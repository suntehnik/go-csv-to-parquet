# Журнал общения и решений

## Вводные
- Задача: разработка библиотеки на Go для потокового преобразования CSV в Parquet (локально и S3), с поддержкой схем, сжатия, простым API и тестами.
- Все требования и ограничения описаны в README.md.

## 2025-06-01

### Запрос пользователя
- Попросил изучить README.md, предложить архитектуру, исследовать библиотеки, зафиксировать решения в ARCHITECTURE.md, вести подробный журнал (CONVERSATION.md).

### Действия и решения
- Провёл исследование популярных Go-библиотек для Parquet:
  - xitongsys/parquet-go (выбран как основной)
  - segmentio/parquet-go (устарел, архивирован)
  - fraugster/parquet-go (ограничен по функционалу)
  - apache/arrow (слишком сложен для задачи)
- Для работы с S3 — aws/aws-sdk-go-v2 (официальный SDK)
- Для CSV — стандартная encoding/csv
- Для YAML — gopkg.in/yaml.v3
- Зафиксировал архитектуру и обоснование выбора библиотек в ARCHITECTURE.md
- Сформулировал уточняющие вопросы:
  1. Какие типы данных должны поддерживаться (int, float, bool, string, date...)?
  2. Нужно ли поддерживать вложенные структуры (nested records) или только flat?
  3. Требуется ли поддержка пользовательских схем или auto-detect достаточно?
  4. Какие требования к производительности (размеры файлов, batch size)?
  5. Нужно ли поддерживать другие облака кроме S3?

---

## 2025-06-01 — Уточнение требований пользователя

**Ответы пользователя:**
1. Требуется поддержка всех типов данных (int, float, bool, string, date и др.)
2. Вложенные структуры не требуются (только flat)
3. Только пользовательская схема, но с возможностью добавить автоопределение схемы в будущем
4. Ожидается работа с очень большими файлами (сотни миллионов строк), максимальная скорость — приоритет
5. Только S3, но архитектура должна позволять расширение на другие облака в будущем

**Выводы для архитектуры:**
- Схема будет задаваться пользователем (YAML), авто-дете́кт — в будущем.
- Паркет-слой должен поддерживать все основные типы Parquet (см. parquet-go: INT32, INT64, FLOAT, DOUBLE, BOOLEAN, BYTE_ARRAY/UTF8, DATE и др.).
- Только flat-структуры — упрощает маппинг и ускоряет обработку.
- Особое внимание — на стриминг, минимизацию памяти и batch-процессинг.
- S3-адаптер проектируется с учётом возможности расширения (интерфейс StorageAdapter).

---

## 2025-06-01 — Изменение решения пользователя

**Пользователь выбрал:**
- Основная библиотека для Parquet: **apache/arrow/go** (официальная от Apache)

**Причины и последствия:**
- Официальная поддержка Arrow/Parquet, богатая экосистема, кросс-языковая совместимость.
- Ожидается высокий порог вхождения, потребуется глубокое изучение API и интеграция.
- Возможны сложности с документацией и отсутствием Go-ориентированных примеров.
- Архитектура и дальнейшие шаги будут строиться вокруг Arrow Go.

---

### 2025-06-01 — Пересмотр архитектуры под apache/arrow/go

**Ключевые изменения:**
- Основная схема данных — Arrow Schema (не Go-структуры).
- Поток: YAML-схема → Arrow Schema → CSVReader → Arrow RecordBatch → ParquetFileWriter → StorageAdapter (S3/Local).
- Batch-обработка реализуется через Arrow RecordBatch фиксированного размера.
- Нет генерации Go-структур через reflect, вся работа с типами — через Arrow.
- StorageAdapter должен поддерживать потоковую запись для S3 и локального диска.

**Вызовы и риски:**
- Высокий порог вхождения, мало Go-примеров, сложная документация.
- Необходимость ручного маппинга типов YAML→Arrow.
- Возможные сложности с типами, сериализацией и Arrow API.
- Требуется профилирование на больших данных.

---

### 2025-06-01 — Решение по конвертеру схемы (YAML→Arrow)

**Почему свой конвертер:**
- В Go-реализации Apache Arrow нет поддержки человеко-читаемых схем (YAML/JSON) "из коробки".
- Официального Arrow-JSON-стандарта нет, Python/R-решения не совместимы с Go и не покрывают все типы.
- Для гибкости, контроля и расширяемости необходим собственный парсер YAML→Arrow Schema.
- Это позволит реализовать валидацию, строгий маппинг и подготовить базу для будущих расширений (например, авто-детект схемы).

---

### 2025-06-01 — Пересмотр плана: жёсткий TDD

**Что изменилось:**
- План реализации полностью переписан с акцентом на строгий TDD.
- Для каждого этапа сначала проектируются и реализуются тесты (unit/integration), только затем пишется код.
- Для каждого компонента и этапа заранее фиксируются критерии готовности (Definition of Done) и тестовые сценарии.
- Definition of Done: все тесты зелёные, покрытие edge-cases, критерии готовности выполнены.

_Дальнейшие шаги и обсуждения будут фиксироваться здесь._
