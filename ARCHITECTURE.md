# Архитектура библиотеки CSV → Parquet на Go

## 1. Обзор задачи
Библиотека предназначена для потокового (batch/stream) преобразования CSV-файлов в формат Parquet с поддержкой локальных и S3-хранилищ, пользовательских схем, сжатия и удобного API для интеграции в проекты на Go. Требования и ограничения подробно описаны в README.md.

## 2. Основные компоненты архитектуры

- **Options** — структура параметров конвертации (пути, схема, сжатие, batch size и др.)
- **CSVReader** — интерфейс потокового чтения CSV (реализации: LocalCSVReader, S3CSVReader)
- **SchemaLoader** — загрузка и валидация схемы из YAML (опционально)
- **SchemaDetector** — автоматическое определение схемы по данным CSV
- **TypeMapper** — сопоставление типов CSV с типами Parquet
- **ParquetWriter** — интерфейс для потоковой записи в Parquet (реализации: LocalParquetWriter, S3ParquetWriter)
- **StorageAdapter** — абстракция доступа к локальным и S3-файлам
- **Convert** — glue-код, orchestration: связывает все компоненты в единую функцию

## 3. Выбор библиотек и анализ

### Для работы с Parquet:

#### 1. [xitongsys/parquet-go](https://github.com/xitongsys/parquet-go)
- **Плюсы:**
  - Самая зрелая и функциональная go-библиотека для Parquet
  - Поддержка записи/чтения структур, JSON, CSV, Arrow
  - Потоковая запись, поддержка сжатия (snappy, gzip, zstd)
  - Активно поддерживается, много примеров
- **Минусы:**
  - Не всегда идеальная обратная совместимость между версиями
  - Не самая простая API для динамических схем (требует генерации struct через reflect)

#### 2. [segmentio/parquet-go](https://github.com/segmentio/parquet-go)
- **Плюсы:**
  - Высокая производительность
  - Чистый, минималистичный API
- **Минусы:**
  - Проект архивирован, не развивается
  - Нет поддержки Arrow, меньше примеров, не рекомендуется для новых проектов

#### 3. [fraugster/parquet-go](https://github.com/fraugster/parquet-go)
- **Плюсы:**
  - Простота, поддержка базовых типов
  - Поддержка некоторых инструментов для инспекции файлов
- **Минусы:**
  - Ограниченный функционал, мало примеров
  - Не подходит для продвинутых сценариев

#### 4. [apache/arrow/go](https://github.com/apache/arrow-go)
- **Плюсы:**
  - Официальная поддержка Arrow/Parquet от Apache
  - Богатая экосистема, поддержка многих языков
- **Минусы:**
  - Сложная интеграция, избыточна для простой задачи CSV→Parquet
  - Высокий порог вхождения, мало Go-примеров

### Для работы с S3:
- [aws/aws-sdk-go-v2](https://github.com/aws/aws-sdk-go-v2) — современный, поддерживаемый SDK для доступа к S3
- [minio/minio-go] — альтернатива для S3-совместимых хранилищ

## 4. Сравнительная таблица Go-библиотек для Parquet

| Критерий                | xitongsys/parquet-go | segmentio/parquet-go | fraugster/parquet-go | apache/arrow/go |
|-------------------------|:-------------------:|:-------------------:|:-------------------:|:---------------:|
| **Поддержка типов**     | Все основные        | Ограниченно         | Ограниченно         | Все             |
| **Стриминг**            | Да                  | Частично            | Нет                 | Да              |
| **Сжатие**              | Да (snappy, gzip, zstd) | Да (частично)   | Нет                 | Да              |
| **S3**                  | Да (через интерфейс) | Нет                 | Нет                 | Да              |
| **Документация**        | Хорошая, много примеров | Средняя         | Ограничена          | Сложная         |
| **Активность**          | Высокая             | Нет                 | Низкая              | Очень высокая   |
| **Дата последнего коммита** | 2024-08-13         | 2023-07-12          | 2022-08-18          | 2025-05-31      |
| **Открытых issues за год** | ~16                | 0                   | 0                   | >100            |
| **Простота API**        | Средняя (reflect/struct) | Простая         | Простая             | Сложная         |
| **Поддержка Arrow**     | Да (ArrowWriter)    | Нет                 | Нет                 | Да (ядро)       |
| **Batch-обработка**     | Да                  | Да                  | Нет                 | Да              |
| **Пользовательские схемы** | Да (reflect/struct/tag) | Нет           | Нет                 | Да              |
| **Производительность**  | Высокая             | Высокая             | Средняя             | Высокая         |
| **Статус проекта**      | Production          | Архивирован         | Поддерживается      | Production      |
| **Примеры**             | Много               | Несколько           | Мало                | Мало            |

### Анализ активности
- **xitongsys/parquet-go**: Последний коммит — август 2024, за год открыто 16+ issues, обсуждения и пулреквесты идут регулярно. Активное сообщество, поддержка новых фич и багфиксов.
- **segmentio/parquet-go**: Последний коммит — июль 2023, проект архивирован, новых issues и активности нет.
- **fraugster/parquet-go**: Последний коммит — август 2022, за год новых issues нет, активность низкая.
- **apache/arrow/go**: Последний коммит — май 2025, за год открыто >100 issues, огромная активность, но большая часть — не Go, а Python/C++/R.

### Краткие выводы
- **xitongsys/parquet-go** — единственная Go-библиотека, поддерживающая все требования: стриминг, сжатие, S3, все типы, batch, Arrow, пользовательские схемы, активное развитие.
- **segmentio/parquet-go** — архивирован, не рекомендуется для новых проектов.
- **fraugster/parquet-go** — очень базовая, не подходит для сложных сценариев и больших объёмов.
- **apache/arrow/go** — мощная, но слишком сложна и избыточна для задачи CSV→Parquet, мало Go-примеров.

## 5. Итоговый выбор
- **Parquet:** apache/arrow/go (официальная библиотека от Apache — выбран пользователем, несмотря на сложность и высокий порог вхождения)
- **S3:** aws/aws-sdk-go-v2
- **Парсинг CSV:** стандартная библиотека encoding/csv
- **YAML:** gopkg.in/yaml.v3

> **Внимание:** Apache Arrow Go — это мощная, но сложная и многокомпонентная экосистема. Для задач CSV→Parquet потребуется глубокое изучение API, настройки зависимостей и интеграции. Ожидается высокий порог вхождения, но открываются широкие возможности для совместимости с Arrow/Parquet-стеком на всех языках.

### Причины выбора
- Официальная поддержка формата Parquet и Arrow.
- Богатая экосистема и кросс-языковая совместимость.
- Перспектива долгосрочной поддержки и развития.
- Возможность интеграции с Python, Java, R и др.
```
[User YAML Schema]
        |
   [SchemaLoader]
        |
     [CSVReader] ---> [ArrowRecordBatchBuilder]
        |                   |
        |                [Batch]
        |                   |
   [ArrowTable/RecordBatch] --> [ParquetFileWriter]
        |
  [StorageAdapter] (Local/S3)
```

- **SchemaLoader**: Преобразует YAML-схему пользователя в Arrow Schema.

#### Почему собственный конвертер схемы (YAML→Arrow)
- В Go-реализации Apache Arrow отсутствует поддержка автоматического создания схемы из YAML/JSON или другого человеко-читаемого формата.
- Официального стандарта Arrow-JSON для схем нет, а существующие решения в Python/R не совместимы с Go и не поддерживают все типы.
- Поэтому необходим собственный парсер, который будет валидировать YAML, сопоставлять пользовательские типы с Arrow Field/Type и строить Arrow Schema программно.
- Такой подход обеспечивает максимальную гибкость, контроль над ошибками и расширяемость (например, для будущей поддержки авто-детекта схемы или других форматов).
- SchemaLoader становится критически важным компонентом архитектуры, отвечающим за корректность и надёжность всего пайплайна.
- **CSVReader**: Стриминг чтения CSV, преобразование строк в Arrow values.
- **ArrowRecordBatchBuilder**: Формирует batch-ы Arrow RecordBatch согласно Arrow Schema.
- **ParquetFileWriter**: Использует Arrow Table/RecordBatch для записи в Parquet (через Arrow API).
- **StorageAdapter**: Унифицированный интерфейс для записи файлов (локально или в S3).
- Batch-обработка и стриминг реализуются на уровне Arrow RecordBatch.
- Нет генерации Go-структур через reflect: используется Arrow Schema.
- Вся обработка типов и маппинг — через Arrow типы.

### Ключевые изменения и вызовы
- Архитектура строится вокруг Arrow Table/RecordBatch, а не Go-структур.
- Необходимо реализовать преобразование CSV-строк в Arrow values с учётом типов.
- Batch-обработка: формирование Arrow RecordBatch фиксированного размера для минимизации памяти.
- Запись в Parquet — только через Arrow API.
- StorageAdapter должен поддерживать потоковую запись в S3 и локально.
- Возможны сложности с ошибками типов, сериализацией и поддержкой всех Parquet-типов.

### Риски и особенности
- Высокий порог вхождения: сложная документация, мало Go-примеров.
- Возможны изменения API Arrow/Go.
- Необходимость ручного маппинга типов из YAML в Arrow.
- Возможные ограничения по производительности при больших файлах (требуется профилирование).

## 6. Краткое описание потоков данных
- Convert инициализирует компоненты по Options
- CSVReader читает batch-ами строки CSV
- SchemaLoader формирует схему для Parquet
- ArrowRecordBatchBuilder преобразует CSV в Arrow RecordBatch
- ParquetFileWriter пишет batch-ами в Parquet-файл
- StorageAdapter абстрагирует работу с локальными/S3 файлами

## 7. Причины выбора
- apache/arrow/go — единственная Go-библиотека, поддерживающая все требования: стриминг, сжатие, S3, все типы, batch, Arrow, пользовательские схемы, активное развитие.
- aws-sdk-go-v2 — официальный SDK для S3, поддержка IAM, multipart upload, стабильность
- Остальные решения либо устарели, либо не покрывают все требования

## 8. Возможные риски и пути их снижения
- Сложности с динамическими схемами в parquet-go — покрыть тестами, использовать reflect
- Ограничения S3 API — реализовать адаптер через io.Reader/io.Writer
- Производительность — batch-обработка, минимизация копирований

## 9. Пример интерфейсов (Go)

```go
// CSVReader
ReadBatch() ([][]string, error)
Close() error

// ParquetWriter
WriteBatch([]map[string]string) error
Close() error
```

---

**Вопросы для уточнения:**
1. Какие типы данных должны поддерживаться (int, float, bool, string, date...)?
2. Нужно ли поддерживать вложенные структуры (nested records) или только flat?
3. Требуется ли поддержка пользовательских схем или auto-detect достаточно?
4. Какие требования к производительности (размеры файлов, batch size)?
5. Нужно ли поддерживать другие облака кроме S3?

---

_Подробный журнал общения ведётся в файле CONVERSATION.md_
